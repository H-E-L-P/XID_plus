% mn2eguide.tex
% v2.1 released 03/05/2002
%
% Adapted from mnguide.tex
% v1.3 released 14th September 1995
% v1.2 released 5th September 1994 (M. Reed)
% v1.1 released 18th July 1994
% v1.0 released 28th January 1994


% The journal style files and macros, with guides on their use, are
% available by anonymous FTP on the Internet from the Comprehensive
% TeX Archive Network (CTAN) sites ftp.tex.ac.uk and ftp.dante.de.
% The files are in the directories
% /tex-archive/macros/plain/contrib/mnras and
% /tex-archive/macros/latex209/contrib/mnras for the TeX and LaTeX
% files respectively.



\documentclass[useAMS,usenatbib]{mnras}

\usepackage{rotating}
\usepackage{lscape}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{epstopdf}
\usepackage{color}
\usepackage{soul}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{textcomp}
\usepackage[caption=false]{subfig}
\usepackage{float}
\usepackage{appendix}
\usepackage{listings}

\def\mnras{MNRAS}
 \def\apj{Astrophys. J.}
 \def\aap{Astron. Astrophys.}
 \def\apjs{Astrophys. J., Suppl. Ser.}



%--------------------------------------------------------
\bibliographystyle{mn2eNicola}

\usepackage{hyperref}

\title[HELP:XID+]
  {HELP: XID+, The Probabilistic De-blender for Herschel SPIRE maps}\author[P.D. Hurley et al.]{P.D.~Hurley,$^1$\thanks{Email: p.d.hurley@sussex.ac.uk} S.~Oliver,$^1$ and other HELP team members\\
$^1$Astronomy Centre, Department of Physics and Astronomy, University of Sussex, Falmer, Brighton BN1 9QH, UK\\}

\date{Released 2002 Xxxxx XX}

\pagerange{\pageref{firstpage}--\pageref{lastpage}} \pubyear{2002}

\def\LaTeX{L\kern-.36em\raise.3ex\hbox{a}\kern-.15em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}

\newtheorem{theorem}{Theorem}[section]
\graphicspath{}
\begin{document}

\label{firstpage}
\maketitle

\begin{abstract}
The Herschel Extragalactic Legacy Project (HELP) will provide ancillary data from other wavelengths alongside the extra
Software available at \url{https://github.com/pdh21/XID_plus/}.
\end{abstract}


\begin{keywords}
galaxies: statistics -- infrared: galaxies
\end{keywords}
%
%
\section{Introduction}
%Infrared astronomy
Ever since the discovery of the far-infrared background with the \emph{Cosmic Background Explorer} (COBE) \citep{Puget:1996}, surveys have aimed to observe and detect the sources responsible for the emission. Most of those sources are galaxies, with the far-infrared emission coming from dust. 

While ground based observatories such as SCUBA, and more recently SCUBA2 and ALMA can make use of gaps in the infrared window to observe at the Rayleigh-Jeans tail of the far-infrared background, only space based facilities can observe at the peak. The first infrared space telescope, the InfraRed Astronomical Satellite (IRAS; \cite{Neugebauer:1984}), observed the whole sky in four bands centred at 12, 25, 60 and 100 $\mu m$ and revealed new populations of galaxies which were optically faint but luminous in the infrared \citep{Soifer:1984}.

While the Infrared Space Observatory (ISO; \cite{Kessler:1996}) and the Spitzer Space Telescope \citep{Werner:2004} have provided deep near and mid-infrared photometry over small fields, other smaller space based facilities such as AKARI \citep{Murakami:2007} and the Wide-field Infrared Survey Explorer (WISE, \cite{Wright:2010}) have surveyed the entire sky at mid to far-infrared and near to mid infrared wavelengths respectively. The most recent advancement in infrared astronomy has been made with the ESA Herschel Space Observatory \citep{Pilbratt:2010}. Photometry from the PACS and SPIRE instruments has given an unprecedented view of the far-infrared Universe by providing observations that measure across the peak of the far-infrared background and at greater sensitivity and resolution than has been achieved at these wavelengths.

With surveys such as the Herschel Multi-Tiered Extragalactic Survey (HerMES; \cite{Oliver:2012}) and the Herschel ATLAS survey (HATLAS; \cite{}), over 1000 square degrees of the sky has been observed by the SPIRE instrument. However, the limited resolution of the SPIRE photometer and high source density at the SPIRE wavelengths introduces two problems when trying to combine SPIRE data with existing multi wavelength data. 

The large beam size and low signal to noise of the SPIRE data results in large positional uncertainties for SPIRE detected sources. This makes matching to catalogues at other wavelengths difficult, as there may be more than one potential counterpart within the positional errors of the SPIRE detected source.

The second problem is that of source confusion. At the wavelengths observed by SPIRE, there is a high source density. This can often result in multiple sources being located within the SPIRE beam and, in extreme cases, lead to multiple astronomical sources being associated with one SPIRE detected source. 

To obtain accurate photometry from the SPIRE maps, overcoming the source confusion problem is essential. One way to solve the problem is to use prior information to accurately separate out the flux in the SPIRE maps to the underlying astronomical objects. For example, if we know the precise location of a galaxy (e.g. from an optical image where resolution is better), we expect a galaxy to be found in the SPIRE maps at the same location.

Several techniques have been developed that utilise the positions of sources detected at other wavelengths, usually 24 $\mathrm{\mu m}$ and 1.4 GHz, to disentangle the various contributions from discrete sources to the SPIRE flux in a given beam element \citep[e.g.][]{Roseboom:2010, Roseboom:2011, Chapin:2011}. This process is made possible by the high correspondence between the 24-$\mathrm{\mu m}$ and 1.4-GHz populations and those observed at far-IR wavelengths; $>$80 per cent of the cosmic IR background at SPIRE wavelengths can be accounted for by 24-$\mathrm{\mu m}$ sources with S24 $>$ 25 $\mathrm{\mu Jy}$ \citep{Marsden:2009, Pascale:2009}, while the strong correlation between the far-IR and radio luminosity is known to hold across a wide range in redshift and luminosity \citep[e.g.][]{Ivison:2010}.

The 24 $\mathrm{\mu m}$ XID algorithm, used by HerMES \citep{Roseboom:2010, Roseboom:2011} used a source list taken from the 24-$\mathrm{\mu m}$ source catalogue to introduce positional priors for sources in the 250, 350 and 500 $\mathrm{\mu m}$ maps. The same method was also used to produce HerMES DR2 source catalogues, with 250 $\mathrm{\mu m}$ sources as priors \citep[DESPHOT]{Wang:2014}. 

 As part of the \emph{Herschel Extragalactic Legacy Project} (HELP), we have developed an alternative prior based approach for source extraction in confusion dominated maps. Our new method, \emph{XID+}, is built upon a Bayesian probabilistic framework which provides a natural framework in which to introduce additional prior information. By using the Bayesian inference tool, \citep[\emph{Stan}]{}, to sample full posterior distribution, we are also able to provide more accuracy flux estimates than \emph{DESPHOT}, whilst avoiding some of the issues associated with the maximum likelihood and L1-constrained fitting approach used by \emph{DESPHOT}.
 
\section{XID+ Algorithm}
The basic goal of XID+ is to use the SPIRE maps to infer the likely SPIRE flux of sources we already know about. Bayesian inference is well suited to these requirements. It allows the use of prior information and provides a posterior distribution of the parameter(s) after taking into account the observed data.  

As discussed previously, we also want to provide a framework to do science directly with the maps rather than adding the additional step of first creating catalogues, which in essence is a form of data compression.

We therefore adopt a Bayesian probabilistic modelling approach for our XID+ algorithm. It aims to:
\begin{itemize}
\item map out the posterior rather than the traditional maximum likelihood point estimate, thereby providing a precise measure of uncertainty. 
\item Extend the use of prior information beyond just using positional information about sources.
\end{itemize}

In the following section, we describe our XID+ algorithm. As this algorithm is meant to build upon knowledge gained from the original XID (a.k.a DESPHOT) algorithm used by HerMES \citep{Roseboom:2010, Roseboom:2011, Wang:2014}, we describe XID+ in the context of how it differs from DESPHOT. 

%change ordering: 
%Basic equations--linear fit--DESPHOT(LASSO)--XID+ model (model and STAN)--
%segmentation:-- optimum--DESPHOT--XID+
%uncertianties and covariances: DESPHOT---XID+

\subsection{Basic Model}
Our data $\mathbf{d}$ are maps with $n_1 \times n_2 = M$ pixels. Our model assumes the map are formed from $n$ known point sources, with an unknown flux density $\mathbf{f}$. The point response function (PRF) tells us the contribution each source makes to each pixel in the map. As the SPIRE maps are mean subtracted, our model requires a global background level ($B$) and some unknown noise term ($\delta$). Our map can therefore be described as follows:

\begin{equation}
\mathbf{d} = \sum\limits_{i=1}^n \mathbf{P_i}f_i + B + \delta
\label{eq:map}
\end{equation}
where $\mathbf{d}$ is the image, $\mathbf{P_i}$ is the PRF for source $i$, $f_i$ is the flux density for source $i$, $B$ is a global background estimate and $\delta$ is the noise term.

We can rewrite the above equation in the linear form:
\begin{equation}
\mathbf{d} = \mathbf{Af} + \delta
\label{eq:map2}
\end{equation}
Where $A$ is the pointing matrix, an $M \times n+1$ matrix giving the contribution of sources and background to each pixel.

Our map $\mathbf{d}$ will have an associated, and measurable, variance and possibly covariance between the pixels, which we define here as $\mathbf{N_d} = \langle\delta\delta^T\rangle$. We define the likelihood as the Gaussian probability function for the data given the flux densities
\begin{equation}
L(\hat{\mathbf{f}}) = p(\mathbf{d}|\hat{\mathbf{f}}) \propto |\mathbf{N_d}|^{-1/2} \exp\big\{ -\frac{1}{2}(\mathbf{d}-\hat{\mathbf{d}})^T\mathbf{N_d}^{-1}(\mathbf{d}-\hat{\mathbf{d}})\big\}\label{eq:likelihood}
\end{equation}
where $\hat{\mathbf{d}}=\mathbf{A\hat{f}}$. The maximum likelihood solution to this equation can be found by setting $\chi = (\mathbf{d}-\hat{\mathbf{d}})^T\mathbf{N_d}^{-1}(\mathbf{d}-\hat{\mathbf{d}})$, finding the minimum and rearranging such that:

\begin{equation}
\hat{\mathbf{f}}=(\mathbf{A^TN_d^{-1}A})^{-1}\mathbf{A^TN_d^{-1}A}\label{eq:mlm}
\end{equation}

Equation \ref{eq:mlm} can be solved directly, either by brute-force matrix inversion or via other linear methods. As discussed in \cite{Roseboom:2010, Roseboom:2011, Wang:2014}, linear approaches ignore prior knowledge that fluxes cannot have negative flux density.% which in very degenerative cases can result in any symmetric pairing of positive and negative flux providing a good fit.
They are also incapable of discriminating between real and spurious sources, which can result in overfitting. To overcome these issues, \cite{Roseboom:2011} used the non-negative weighted LASSO algorithm (Tibshirani 1996; Zou 2006; ter Braak et al 2010).

LASSO is a shrinkage and selection method for linear regression and works by treating sources either `inactive' and flux density set to zero, or `active'. It switches sources on one at a time, with the order determined by reduction in chi-squared gained by turning them on. The process continues until some tolerance is reached.

%In the first iteration,\emph{DESPHOT}uses LASSO on each segment, to estimate the source fluxes. It then estimates a value for the background (B) via
%\begin{equation}
%B = \mathbf{d} - \sum\limits_{i=1}^n \mathbf{P_i}f_i
%\end{equation} 
%
%The estimate from B is subtracted, and the LASSO fitting is rerun to get the final flux density estimates.

For XID+, we want to map out the entire posterior, $p(\mathbf{f}|\mathbf{d})$, rather than find the maximum likelihood solution. This has the benefit that it gives us more accurate information about how certain we are about the predicted fluxes. The posterior can be defined as:
\begin{equation}
p(\mathbf{f}|\mathbf{d}) \propto p(\mathbf{d}|\mathbf{f}) \times p(\mathbf{f})
\end{equation}
where $p(\mathbf{d}|\mathbf{f})$ is our likelihood, defined in equation \ref{eq:likelihood} and $p(\mathbf{f})$ is our prior on the fluxes. 

In our probabilistic framework, we can illustrate our model for the map, defined in equation \ref{eq:map2} via a probabilistic graphical model (PGM). Figure \ref{fig:graph_mod_xid+} shows our PGM for our basic XID+ model, where boxes represent dimensions, open circles as random variables, dots as deterministic or (fixed) variables. For our simplest model, the sky co-ordinates of our sources are treated as fixed, as is the PRF. Both these fixed variables are used to make the pointing matrix $A$ which gives the contribution each source makes to each pixel $j$ in the map. Each source has its own flux $f$ which is a random variable. By multiplying $f$, $A$ and adding our global estimate for the background $B$, we can make our model for the map, $M$. 
\begin{figure}
\includegraphics[width=8.5cm]{./graphical_model.pdf}
\caption{Our probabilistic model for XID+. Boxes represent dimensions, open circles as variables, dots as deterministic or (fixed) variables. Created with DAFT (\url{http://daft-pgm.org/})}\label{fig:graph_mod_xid+}
\end{figure}
 
\subsubsection{Stan}
Now we have our probabilistic model, we need a Bayesian inference tool capable of sampling from it to obtain the posterior. We use the Bayesian inference tool, \textit{Stan}, which is `a probabilistic programming language implementing full Bayesian statistical inference with MCMC sampling'. \textit{Stan} uses the adaptive Hamiltonian Monte Carlo (HMC) No-U-Turn Sampler (NUTS) of Hoffman & Gelman (In press) to efficiently sample from the posterior. It does this by using the gradient, which allows fast traversing of high dimensional and highly correlated joint posterior distributions. 

\textit{Stan} has its own modelling language, in which one constructs probabilistic models. Our model for \textit{Stan} can be found in Appendix A.
\subsubsection{Estimating Convergence}\label{sec:conv}
As with all MCMC routines, one needs to run enough chains and run them long enough to be confident the global minimum has been found and that it has been thoroughly sampled. 

As default, we run four separate chains from different initial positions. We also discard the first half of the chain as `warm up' to ensure the chains have converged to the posterior distribution. We then assess the convergence of each parameter by comparing the variation between and within chains using the diagnostics described in \cite{BDA3} which can be summarised as follows: Each chain is split in two and the between ($B$) and within chain ($W$) variance is calculated. $B$ and $W$ are then used to calculate the marginal posterior variance. This in turn can be used to estimate the potential scale reduction $\hat{R}$, which reduces to 1 as the number of iterations tends to infinity. An $\hat{R}$ value $> 1.2$ suggests chains require more samples. We provide $\hat{R}$ for each parameter.

Due to the nature of MCMC, samples from MCMC routines are correlated. Inference from correlated samples is less precise than from the same number of independent draws. In order to check there are enough independent draws we estimate the effective number of samples $\hat{n_{eff}}$, defined in \cite{BDA3}. We require $\hat{n_{eff}}$ to be 5 times the number of chains and provide the estimate for each parameter.

\subsection{Map segmentation}
The survey fields in HELP vary in size from .... to ..... square degrees. Ideally, source photometry and background estimation would be done on the full image, in practice it is often computationally infeasible.\emph{DESPHOT}segmented the map by locating islands of high SNR pixels enclosed by low SNR pixels. The segmentation algorithm operates thus:
\begin{itemize}
\item Locates all pixels with a SNR above some threshold (default value of SNR= 1);
\item Takes the first of these high SNR pixel starting in the bottom left corner of the image;
\item `Grows' a region around this pixel by iteratively taking neighbouring high SNR pixels;
\item Once there are no more high SNR neighbours jumps to the next high SNR pixel and repeat from step (iii).
Each of these independent regions of high SNR pixels is uniquely identified and processed separately by the source photometry component.
\end{itemize}

The most optimum and transparent way of segmenting the map would use distinct tiles but fitting for sources within and beyond the tiling region. Using the fact that, conditional on the fluxes, the data from each tile is independent, the posterior from the different tiles can be combined to give an overall posterior for the whole map using the following formula:

\begin{equation}
P(\mathbf{F}|D_1,..D_n,D_A)=a\frac{\prod_{i=1}^n P(\mathbf{F}|D_n,D_A)}{P(\mathbf{F}|D_A)^{n-1}}
\end{equation}

where $n$ is number of tiles and $P(\mathbf{F}|D_A)$ is our prior on the flux, given some ancillary data. As the number of dimensions is highly dimensional, the only feasible way of multiplying highly dimensional probability distribution functions is to model them as multivariate Gaussians. We investigated fitting the fluxes in both normal and log space, however in neither case was the multivariate Gaussian approximation appropriate.

We have settled on a tiling scheme that has tiles that overlap in the map. This makes sure sources that lie near the edge of a tile, will be nearer the centre of the adjacent tile. As before, for each tile, we fit to sources both in and beyond. For our final posterior, for each source, we take the posterior form the tile that is optimum, i.e. the tile in which the source lies closest to the centre.

\subsection{Uncertainties and Covariances}
If\emph{DESPHOT}is assumed to be linear\footnote{introducing LASSO and non-negative priors introduces a non-linearity}, then one can get a lower limit on the noise from $(\mathbf{A^TN_d^{-1}A})^{-1}$. This estimate only includes instrumental noise and degeneracies between sources. An estimate of the remaining residual confusion noise is is calculated by taking the standard deviation of the residual map pixels $\sigma{res}$ and removing the average instrumental noise in these pixels in quadrature, $\sigma^2_{conf} = \sigma^2_{res} - \sigma^2_{pix}$, where $\sigma_{pix}$ is calculated directly from the exposure time per pixel. The total noise $\sigma_{tot}$ for a point source is then calculated from both the instrumental noise (and confusion noise from the known sources), $\sigma_{i} = \sqrt{\mathrm{diag}((\mathbf{A^TN^{-1}_dA)^{-1})}}$, and confusion noise from the unknown sources in the residual map $\sigma_{conf}$ via $\sigma^2_{tot} = \sigma^2_{i} + \sigma^2_{conf}$. 

%Details on what we get out of fit. e.g. marginalised estimate of fluxes, covariance information between sources
For XID+ we are sampling directly from the posterior which give the uncertainty of the flux given the data. This includes the uncertainty from instrumental noise. Unlike XID, we are not solving $\mathbf{f}=(\mathbf{A^TN_d^{-1}A})^{-1}\mathbf{A^TN_d^{-1}d}$, we are solving equation \ref{eq:map} and so variations in pixel flux from sources not in the prior list, i.e. from confusion, will directly affect our flux estimates. This is advantageous as\emph{DESPHOT}was well known to underestimate the uncertainties.


\section{Simulations}
In order to test and quantify XID+, we use simulated SPIRE maps, generated from the Lacey simulations. NEED MORE DETAIL

\begin{figure*}
\centering 
\subfloat{\includegraphics[width=6cm,page={4}]{metrics_XIDp.pdf}}
\subfloat{\includegraphics[width=6cm,page={5}]{metrics_XIDp.pdf}}
\subfloat{\includegraphics[width=6cm,page={6}]{metrics_XIDp.pdf}}\\
\subfloat{\includegraphics[width=6cm,page={4}]{metrics_DESPHOT.pdf}}
\subfloat{\includegraphics[width=6cm,page={5}]{metrics_DESPHOT.pdf}}
\subfloat{\includegraphics[width=6cm,page={6}]{metrics_DESPHOT.pdf}}
\caption{Precision, or IQR of \emph{XID+} and \emph{DEPSHOT} as a function true flux, for all three SPIRE bands. Red solid and dashed lines shows median and standard deviation respectively. Density of objects is illustrated via }\label{fig:precision}
\end{figure*}

A mock 100 $\mathrm{\mu m}$ input catalogue, similar to that expected from a PACS catalogue, is generated by taking the mock catalogue and making a cut at a flux limit of 50 $\mathrm{mJy}$. We use this as our prior input catalogue for both XID+ and DESPHOT. In order to compare performance, we look at three measures: precision, flux accuracy, and flux error density.

\subsection{Flux Precision}
Precision is a measure of how well the flux is believed to be constrained. For our posterior sample, this relates to how spread out the sample is and so we use the interquartile range as our measure of precision. Figure \ref{fig:precision} show how the precision of XID+ and\emph{DESPHOT}change as a function of input flux for a deep (confusion noise dominated) simulation, for all three bands. 

\subsection{Flux Accuracy}
Flux accuracy is a measure of how far away the estimated flux is from the truth. We use the difference between our median flux estimate from our posterior and the true flux from the simulation, normalised by true flux, as our estimate of flux accuracy. Figure \ref{fig:accuracy} shows how flux accuracy changes as a function of input flux for a deep (confusion noise dominated) simulation, for all three bands. 

\begin{figure*}
\centering 
\subfloat{\includegraphics[width=6cm,page={7}]{metrics_XIDp.pdf}}
\subfloat{\includegraphics[width=6cm,page={8}]{metrics_XIDp.pdf}}
\subfloat{\includegraphics[width=6cm,page={9}]{metrics_XIDp.pdf}}\\
\subfloat{\includegraphics[width=6cm,page={7}]{metrics_DESPHOT.pdf}}
\subfloat{\includegraphics[width=6cm,page={8}]{metrics_DESPHOT.pdf}}
\subfloat{\includegraphics[width=6cm,page={9}]{metrics_DESPHOT.pdf}}
\caption{Accuracy of \emph{XID+} and \emph{DEPSHOT} as a function true flux, for all three SPIRE bands. Red solid and dashed lines shows median and standard deviation respectively. Density of objects is illustrated via }\label{fig:accuracy}
\end{figure*}

\subsection{Flux Error density}
Flux error density measures how well the estimated uncertainty is correct. For well estimated errors, the estimated flux value should be within one sigma of the true value 68.27\% of the time and within 2 sigma 95.45\% of the time. Figure \ref{fig:zscore} shows the flux error density as a function of input flux for \emph{XID+} and \emph{DESPHOT}. For \emph{DESPHOT}, uncertainties are assumed to have a normal distribution, truncated at zero. With \emph{XID+}, we have the full posterior and no longer have to make an assumption on the shape of the uncertainty distribution. Instead, we calculate error density by taking the percentile at which the true flux value falls within the posterior, and express it in terms of the normal sigma level.

\begin{figure*}
\centering 
\subfloat{\includegraphics[width=6cm,page={1}]{metrics_XIDp.pdf}}
\subfloat{\includegraphics[width=6cm,page={2}]{metrics_XIDp.pdf}}
\subfloat{\includegraphics[width=6cm,page={3}]{metrics_XIDp.pdf}}\\
\subfloat{\includegraphics[width=6cm,page={1}]{metrics_DESPHOT.pdf}}
\subfloat{\includegraphics[width=6cm,page={2}]{metrics_DESPHOT.pdf}}
\subfloat{\includegraphics[width=6cm,page={3}]{metrics_DESPHOT.pdf}}
\caption{Z score, or flux density error for \emph{XID+} and \emph{DEPSHOT} as a function true flux, for all three SPIRE bands. Red solid and dashed lines shows median and standard deviation respectively. Density of objects is illustrated via }\label{fig:zscore}
\end{figure*}

\subsection{Convergence}
As described in Section \ref{sec:conv}, we provide $\hat{R}$ as an estimate of convergence and $\hat{n_{eff}}$ as a measure of independence within the sample. Figures \ref{} show the histogram for $\hat{R}$ and $\hat{n_{eff}}$ for the three bands, with our thresholds for the statistics shown by dotted lines. 

It is also vital that our proposed method return reliable estimates of the flux density error, as for real applications we will not have knowledge of the true flux density of our sources. In Figure \ref{Fig:flux_density_error} we show the distribution of observed flux density error (i.e. $S_{obs}-S_{true}$), normalised by the error estimated by the photometric pipeline for the deep simulation. 


\subsection{Remaining Noise}
%Why convolve the residual map with the PSF?: So that each pixel is then estimate of noise from all pixels that contribute to it
%Do I want to add the confusion noise uncertainty to the each source?
%How is my estimate of uncertainty better than DESPHOT? If non gaussian, I capture it, no longer 
 
\subsection{Correlated Sources}
For sources that lie close together, the uncertainty on the flux estimates is correlated between the sources. One of the advantages of obtaining the full posterior is we get a proper estimate of uncertainty and its correlation. This is particularly apparent when comparing with DESPHOT, which, by using the LASSO algorithm would force one source to have all the flux and the other nearby source to zero. Figure \ref{} shows an example two correlated sources sources from the simulation, and the estimate from both \emph{XID+} and \emph{DESPHOT}. The posterior provided by XID+, fully captures the correlated uncertainty, where as the `winner takes all' approach from\emph{DESPHOT}clearly fails to estimate the true flux for both sources.
 

%\subsection{Real data}
% inject sources into map and recover

\section{Conclusions}
In this paper we have introduced the prior based source detection software, \emph{XID+}. By using the Bayesian inference tool \emph{Stan}, we are able to fully sample the posterior probability, which in turn gives a better understanding of the uncertainty associated with the source flux. 

Having run XID+ on simulated maps, we have shown this is extremely advantageous for maps that are confusion limited, such as the \emph{Herschel} observations that are part of \emph{HerMES}. In comparison to the current maximum likelihood based software \emphg{DESPHOT}, XID+ performs far better in all the three main metrics; flux accuracy, precision and uncertainty accuracy.
 
 
By using probabilistic approach, we can introduce prior information on the source fluxes in a transparent way. and obtain the full pos


\section*{Acknowledgements} %
%
%%
%
%
\bibliography{}
\appendix
\section*{Appendix A}\label{Stan_model}
\onecolumn
\lstinputlisting[language=C, linewidth=18cm]{../stan_models/XID+SPIRE.stan}
%
%
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%% \bsp % ``This paper has been produced using the ...''
%
%\label{lastpage}

\end{document}
